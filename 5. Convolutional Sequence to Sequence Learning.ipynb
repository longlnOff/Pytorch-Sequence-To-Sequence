{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Convolutional Sequence to Sequence Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong notebook lần này, chúng ta sẽ implementing model trong paper [Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure1](./images/5.enc_dec.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model trong notebook lần này khác biệt khá nhiều so với models ở các notebook trước. Lần này, chúng ta sẽ không sử dụng RNN để xây dựng model. Thay vào đó, chúng ta sử dụng Convolutional layers, thường được sử dụng trong xử lý ảnh. Tham khảo [link này](https://github.com/longlnOff/sentiment-analysis/blob/main/4.%20Convolutional%20Sentiment%20Analysis.ipynb) để có cái nhìn tổng quan về CNN cũng như cách áp dụng CNN trong bài toán sentiment analysis. Bên cạnh đó, nhấn vào [đây](https://github.com/longlnOff/sentiment-analysis) để tìm hiểu thêm về bài toán sentiment analysis và các phương pháp xử lý."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Về cơ bản, convolutional layers sử dụng các filters. Các filters này có chiều rộng (width) và chiều cao (height). Nếu filter có width bằng 3, nó có thể quan sát 3 tokens liên tiếp. Mỗi convolutional layer có rất nhiều filter (trong notebook này là 1024 filters). Mỗi filter trượt dọc theo sequence, từ đầu tới cuối và quan sát 3 tokens đúng liền kề nhau ở mỗi lần trượt (trong trường hợp filter có width bằng 3). **Ý tưởng chính ở đây là: Mỗi một filter trong 1024 filters sẽ trích xuất ra đặc tính khác nhau từ văn bản. Kết quả của quá trình trích xuất đặc trưng này sẽ model sử dụng làm đầu vào cho các convolutional layers khác. Nhiều lớp convolutional sẽ trích xuất nhiều đặc trưng từ source sentence để dịch nó sang target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import các module cần thiết và thiết lập random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load spacy models và định nghĩa tokenizers cho source sentence và target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta thiết lập `Field`s để xác định cách dữ liệu được xử lý. Mặc định, các RNN models trong Pytorch yêu cầu input sequence là tensor có kích thước **[sequence length, batch size]**, và torchtext cũng mặc định trả về batches các tensor có kích thước tương tự. Tuy nhiên, trong notebook này, chúng ta sử dụng CNNs, và CNNs yêu cầu input là tensor có kích thước **[batch size, sequence length]**. Ta thông báo cho torchtext chuyển tensor mặc định sang tensor có kích thước  **[batch size, sequence length]** bằng cách thiết lập tham số `batch_first = True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bên cạnh đó, chúng ta cũng sẽ thực hiện chuyển các tokens về lower text và chèn token bắt đầu và token kết thúc sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de,\n",
    "            init_token='',\n",
    "            eos_token='',\n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            init_token='',\n",
    "            eos_token='',\n",
    "            lower=True,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta thực hiện load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Build Vocabulary, coi các tokens xuất hiện ít hơn 2 lần trong corpus là \\<unk> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bước cuối cùng trong công đoạn chuẩn bị dữ liệu là xây dựng iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 16\n",
    "train_iterator, valid_iterator, test_iterator = \\\n",
    "        BucketIterator.splits(\n",
    "            (train_data, valid_data, test_data),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta thực hiện xây dựng model. Tương tự các notebook trước, model được tạo bởi 2 khối: encoder và decoder. Khối encoder thực hiện encodes input sentence, ở dạng source language, thành *context vector*. Khối decoder sẽ decodes context vector để sinh ra output sentence ở dạng target language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Models ở các notebook trước sử dụng encoder để nén toàn bộ thông tin của input sentence vào một single context vector,z. Tuy nhiên, convolutional seq2seq model có một chút khác biệt - với mỗi token trong input sequence, ta sẽ thu được 2 context vectors. Nếu input sentence có 6 tokens, ta sẽ thu được 12 context vectors (2 vector cho mỗi token). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hai context vector thu được từ mỗi token là *conved* vector và *combined* vector. *Conved* vector là kết quả của mỗi token khi được truyền qua một vài layers. *Combined* vector là tổng của convolved vector và embedding của token đó. Cả 2  vector này được trả về từ encoder để phục vụ cho quá trình decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hình dưới minh họa việc một input sentence (zwei menschen fetchen) được truyền vào encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure2](./images/5.enc_example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trước tiên, được truyền qua *token embedding layer*. Tuy nhiên, model này không chứa các RNN nên nó không thể nhận biệt được thứ tự của các tokens bên trong sequence. Để khắc phục vấn đề này, ta có một embedding layer thứ 2 - *positional embedding layer*. Đây là một embedding layer chuẩn với đầu vào không phải là tokens, thay vào đó, ta truyền vào vị trí của tokens bên trong sequence - bắt đầu với \\<sos> token, ở vị trí 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, token embeddings và positional embeddings được *elementwise summed* để sinh ra vector chứa thông tin về token cũng như vị trí của token đó bên trong sequence - từ nay ta sẽ gọi vector tổng hợp này là *embedding vector*. Sau đó, ta đưa *embedding vector* này qua một linear layer để biến đối nó thành vector khác có số chiều tương ứng với hidden dimension size mong muốn (đầu ra sẽ là hidden vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bước tiếp theo, ta cần truyền *hidden vector* này vào **N** *convolutionalblocks*. Chi tiết về hoạt động của *convolutional blocks* sẽ được trình bày kỹ hơn ở phần sau. Sau khi đi qua convolutional blocks, vector sẽ được đưa vào một linear layer khác để chuyển nó từ vector có kích thước bằng hidden dimension size sang embedding dimension size. Quá trình trên tạo ra ***conved*** vector - và mỗi token, ta thu được một ***conved*** vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cuối cùng, ***conved*** vector được *elementwise summed* với embedding vector thông qua residual connection để sinh ra ***combined*** vector cho mỗi token. Và mỗi token sẽ có một ***combined*** vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Như vậy, ta thu được 2 vector là: ***conved*** vector và ***combined*** vector đối với mỗi token trong input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Phần này sẽ trình bày ngắn gọn hoạt động của convolutional blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hình dưới mô tả hoạt động của 2 convolutional blocks với một filter (blue) trượt ngang qua các tokens bên trong sequence. Trong notebook này, chúng ta sẽ xây dựng encoder với 10 convolutional blocks, mỗi block có 1024 filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure3](./images/5.convolutional_blocks.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, input sentence được đệm, lý do là convolutional layers sẽ giảm chiều dài của input sentence. Tuy nhiên, ở đây chúng ta mong muốn chiều dài câu đầu vào của convolutional blocks phải bằng với chiều dài output sentence của convolutional blocks. Nếu như không padding, chiều dài output sentence của convolutional blocks sẽ là: \"**chiều dài câu đầu vào - `filter_size - 1`**\", ngắn hơn sequence được đưa vào convolutional layer `filter_size -1` đơn vị. Ví dụ, filter có size bằng 3 thì output sentence sẽ ngắn hơn input sentence 2 đơn vị. Để khắc phục vấn đề này, ta thực hiện padding sequence ở đầu và cuối, mỗi phía 1 token. Ta có công thức tính toán số token cần đệm ở mỗi phía trong trường hợp filter size là số lẻ như sau: <br>\n",
    ">> `number padding token each side = (filter_size - 1) / 2` <br>\n",
    "\n",
    "> (Ở đây, ta không xét trường hợp filter có size là số chẵn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Các filter này được thiêt kế sao cho output hidden dimension gấp đôi input hidden dimension của chúng. Trong computer vision, hidden dimensions được gọi là *channels*, tuy nhiên trong NLP ta sẽ gọi chúng là hidden dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Câu hỏi đặt ra ở đây là: tại sao chúng ta lại nhân đôi kích thước đầu ra cua convolutional filter so với đầu vào? Lý do là ở đây, chúng ta sử dụng một activation function đặc biệt có tên là *gated linear units* (GLU). GLUs tương tự như LSTM hay GRU, đều có cơ chế *gating*. GLUs sẽ trả về đầu ra có kích thước giảm một nửa => chúng ta cần nhân đôi đề đầu vào và đầu ra có kích thước bằng nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau khi đi qua GLUs activation function, hidden dimension size của mỗi token là bằng nhau trước và sau khi đi qua convolutional blocks. Tiếp theo, chúng được *elementwise summed* với chính vector của nó trước khi được truyền vào convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trên đây là hoạt động của single convlutional block. Các blocks tiếp theo có input là output của previous block và thực hiện công việc tương tự. Mỗi block có một bộ parameters riêng của nó, các bộ parameters này không được shared giữa các blocks. Output của last block sẽ quay trở lại encoder và được đưa vào linear layer để sinh ra *conved* vector, sau đó được *elementwise summed* với embedding của token để sinh ra *combined* vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Để đơn giản hóa, ta chỉ sử dụng filter có size là số lẻ - việc này cho phép padding source sentence ở cả 2 phía bằng nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Biến `scale` được sử dụng bởi tác giả để \"đảm bảo phương sai của network không bị thay đổi quá nhiều\". Hiệu suất của model sẽ tay đổi khá nhiều nếu sử dụng seed random khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Positional embedding được khởi tạo để xử lý lý \"vocabulary\" có kích thước bằng 100. Có nghĩa là nó có thể xử lý sequences có chiều dài tối đa 100 tokens, từ vị trí 0 tới 99. Ta có thể tằng kích thước này nếu sequence có chiều dài lớn hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, src len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, src len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, src len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved = [batch size, src len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decoder đọc actual target sentence và cố gắng predict nó. Decoder model có sự khác biệt so với RNN models được sử dụng từ trước - lần này, decoder thực hiện predict tất cả các token trong một lần, quá trình predict các tokens được thực hiện song song với nhau. Không có sequential processing, không có decoding loop, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decoder khá giống với encoder, với một chút thay đổi về convolutional block bên trong model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure4](./images/5.dec.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, embeddings không có residual connection đảm nhận việc kết nối sau convolutional blocks và transformation. Thay vào đó, embedding được truyền vào convolutional blocks để được sử dụng giống như một residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thứ 2, để truyền thông tin từ encoder sang decoder, encoder conved và combined output được sử dụng bên trong convolutional blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cuối cùng, output của decoder là một linear layer chuyển từ embedding dimension sang output dimension. Nó được sử dụng để đưa ra dự đoán về từ tiếp theo trong quá trinh dịch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder Convolutional Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Convolutional blocks bên trong encoder cũng có một vài thay đổi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure5](./images/5.dec_conv.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên là padding. Thay vì padding ở 2 phía của sentence, chúng ta chỉ padding vào đầu của sentence. Do chúng ta đang sử dụng đồng thời tất cả các targets một cách song song (không phải tuần tự), nên chúng ta cần một phương pháp cho phép filters dịch token thứ i chỉ được quan sát các tokens nằm trước từ thứ i. Nếu các filters quan sát được từ phía sau, chúng sẽ sao chép các kết quả và không học được gì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cùng quan sát việc đệm ký tự ở hai phía cho kết quả SAI như thế nào:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ![figure](./images/5.incorrectly.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Filter ở vị trí đầu tiên đang sử dụng từ thứ nhất trong sequence (\\<sos> token) để dự đoán từ thứ hai (từ two), bây giờ có thể trực tiếp quan sát từ thứ 2 (từ two). Quá trình trên xảy ra ở tất cả các vị trí, từ mà model cố gắng dự đoán là phần từ thứ 2 được chính filter của model quan sát (giống như bạn làm bài tập mà biết trước lời giải => khả năng cao là sẽ không học được gì). Việc quan sát trước kết quả khiến filters chỉ đơn giản là sao chép từ ở kết quả mà không học được gì từ việc dịch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thứ hai, sao GLU activation function và trước residual connection, các khối tính toán được áp dụng attention - sử dụng biểu diễn đã được encoded và embedding của từ hiện tại. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure5](./images/5.dec_conv.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** lưu ý là hình trên chỉ thể hiện việc tính attention cho các tokens ở ngoài cùng bên phải, nhưng thực chất, nó được kết với tất cả các tokens. Mỗi token CHỈ sử dụng embedding của chính token đó để tính attention của chính nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Attention được tính bằng cách: đầu tiên, sử dụng một linear layer để thay đổi hidden dimension sao cho có cùng kích thước với embedding dimension. Sau đó cộng embedding bằng residual connection.  This combination then has the standard attention calculation applied by finding how much it \"matches\" with the encoded conved and then this is applied by getting a weighted sum over the encoded combined. This is then projected back up to the hidden dimenson size and a residual connection to the initial input to the attention layer is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vậy tại sao chúng ta cần tính attention trước với encoded conved và sau đó sử dụng nó để tính tổng trọng số thông qua việc kết hợp? Dưới đây là 2 vấn đề mà paper đang cân nhắc: <br>\n",
    ">> * encoded conved rất tốt cho việc lấy bối cảnh tổng quan thông qua encoded sequence.\n",
    ">> * Trong khi đó, encoded combined chứa nhiều thông tin về một token cụ thể hơn, từ đó rất phù hợp cho việc dự đoán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Do trong bài này, chúng ta chỉ thực hiện padding một phía nên decoder được cho phép sử dụng padding có kích thước lẻ hoặc chẵn. Và, `scale` được sử dụng để giảm phương sai của model. Position embedding được khởi tạo với kích thước vocabulary bằng 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model nhận vào biểu diễn của encoder trong `forward` method, và cả hai được áp dụng `calculate_attention` để tính và áp dụng attention. Model có trả về giá trị attention nhưng ở dây, ta không sử dụng chúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 emb_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 kernel_size, \n",
    "                 dropout, \n",
    "                 trg_pad_idx, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        \n",
    "        #conved_emb = [batch size, trg len, emb dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        \n",
    "        #combined = [batch size, trg len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        \n",
    "        #energy = [batch size, trg len, src len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        \n",
    "        #attention = [batch size, trg len, src len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        \n",
    "        #attended_encoding = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        \n",
    "        #attended_combined = [batch size, hid dim, trg len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = [batch size, trg len, emb dim]\n",
    "        #pos_embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        \n",
    "        #conv_input = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        \n",
    "        #conv_input = [batch size, hid dim, trg len]\n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "        \n",
    "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "\n",
    "            #conved = [batch size, 2 * hid dim, trg len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #calculate attention\n",
    "            attention, conved = self.calculate_attention(embedded, \n",
    "                                                         conved, \n",
    "                                                         encoder_conved, \n",
    "                                                         encoder_combined)\n",
    "            \n",
    "            #attention = [batch size, trg len, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            \n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "         \n",
    "        #conved = [batch size, trg len, emb dim]\n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Module đóng gói, `Seq2Seq` có khá nhiều khác biệt so với module Seq2Seq khi áp dụng RNN, đặc biệt là phần decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Biến `trg` có phần tử \\<eos> được cắt bỏ ở cuối do chúng ta không truyền \\<eos> token vào decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quá trình encoding khá giống với các notebook trước, đưa input sentence và nhận lại context vector. Tuy nhiên ở đây chúng ta có tới 2 context vector cho mỗi từ trong source sequence, `encoder_conved` và `encoder_combined`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quá trình decoding được thực hiện song song, do đó chúng ta không cần decoding loop. Tất cả các target sequence đều được đưa vào decoder cùng một lúc và việc padding được sử dụng để đảm bảo mỗi convolutional filter trong decoder chỉ có thể quan sát token hiện tại và token phía trước khi nó trượt trong sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Điều này có nghĩa là chúng ta không áp dụng teacher forcing trên model này. Chúng ta không cần vòng lặp khi predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len - 1] ( token sliced off the end)\n",
    "           \n",
    "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        #encoder_conved is output from final encoder conv. block\n",
    "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
    "        #  positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "            \n",
    "        #encoder_conved = [batch size, src len, emb dim]\n",
    "        #encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #calculate predictions of next words\n",
    "        #output is a batch of predictions for each word in the trg sentence\n",
    "        #attention a batch of attention scores across the src sentence for \n",
    "        #  each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #attention = [batch size, trg len - 1, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Phần còn lại khá giống với các notebook trước, ta thực hiện define các hyperparameters, khởi tạo encoder và decoder, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong paper, người ta đã khảo sát và thấy rằng việc sử dụng nhiều filter có kích thước nhỏ cho kết quả tốt hơn (ở đây dùng nhiều hơn 5 filter, mỗi filter có kích thước bằng 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
    "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
    "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
    "ENC_KERNEL_SIZE = 3 # must be odd!\n",
    "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
    "ENC_DROPOUT = 0.25\n",
    "DEC_DROPOUT = 0.25\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "    \n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta định nghĩa optimizer và loss function. Giống các notebook trước, chúng ta ignore loss khi target sequence là padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau đó là training loop cho model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta xử lý sequences hơi khác so với các notebook trước. Với các model dùng RNN, chúng ta không bao giờ đưa \\<eos> vào decoder. Trong model này, chúng ta chỉ đon giản loại bỏ \\<eos> trong sequence. Do đó: <br>\n",
    ">>> trg = [sos, $x_1$, $x_2$, $x_3$, eos] <br>\n",
    ">>> trg[:-1] = [sos, $x_1$, $x_2$, $x_3$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở đây, $x_i$ là các phần tử trong target sequence. Chúng ta sẽ đưa chúng vào trong model để dự đoán: <br>\n",
    ">>> output = [$y_1$, $y_2$, $y_3$, eos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $y_i$ là các phần tử trong predicted sequence. Ta tính toán loss thông qua tensor gốc `trg` sau khi loại bỏ \\<sos> token, để lại \\<eos> token: <br>\n",
    ">>> output = [$y_1$, $y_2$, $y_3$, eos] <br>\n",
    ">>> trg[1:] = [$x_1$, $x_2$, $x_3$, eos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta thực hiện tính loss và cập nhật tham số như bình thường."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hàm tính thời gian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 0.1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/tut5-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hàm `translate_sentence` sẽ thực hiện các công việc sau:\n",
    ">> * Đảm bảo model ở evaluation mode (inference thì model luôn luôn phải ở chế độ evaluation).\n",
    ">> * Tokenize source sentece nếu nó chưa được tokenized.\n",
    ">> * Chèn \\<sos> và \\<eos> tokens.\n",
    ">> * Số hóa source sentence.\n",
    ">> * Convert source sentence đã được số hóa thành tensor và thêm vào batch dimension.\n",
    ">> * Đưa source sentence vào encoder.\n",
    ">> * Tạo list để lưu trữ output sentence, được khởi tạo với một token \\<sos>.\n",
    ">> * Lặp khi chưa đạt đến độ dài tối đa:\n",
    ">>>> * convert câu dự đoán hiện tại thành tensor với batch dimension.\n",
    ">>>> * Đưa current output và 2 outputs từ encoder vào decoder.\n",
    ">>>> * Lấy giá trị dự đoán của token tiếp theo.\n",
    ">>>> * Thêm giá trị dự đoán vào output sentence prediction.\n",
    ">>>> * break nếu prediction là \\<eos> token.\n",
    "> * Chuyển output sentence từ indexes sang tokens.\n",
    "> * Trả về output sentence (sau khi đã removed các \\<sos> token) và attention value của mỗi sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Hàm hiển thị attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "        \n",
    "    attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticklabels(['']+['']+[t.lower() for t in sentence]+[''], \n",
    "                       rotation=45)\n",
    "    ax.set_yticklabels(['']+translation)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bắt đầu dịch, dưới đây là  example trong training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 2\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off  token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đây là notebook đầu tiên chúng ta không sử dụng RNN. Trong phần tiếp theo, chúng ta cùng tìm hiểu `Transformer` model - model này thậm chí còn không dùng đến convolutional layers, chỉ dùng các linear layers và rất rất nhiều attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
