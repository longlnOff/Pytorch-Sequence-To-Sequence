{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook lần này, chúng ta cùng nhau implemmenting (và có điều chỉnh một chút) một *Transformer Model* từ paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Để hiểu thêm về Transformer, nhấn vào 3 đường link sau: [1](https://www.mihaileric.com/posts/transformers-attention-in-disguise/), [2](https://jalammar.github.io/illustrated-transformer/), [3](http://nlp.seas.harvard.edu/2018/04/03/attention.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dưới đây là kiến trúc Transformer model với Encoder (bên trái) và Decoder (bên phải)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure1](./images/6.transformer_model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tương tự như Convolutional Sequence-to-Sequence model, Transformer model không sử dụng recurrence, nó cũng không sử dụng bất kỳ convolutional layers nào. Thay vào đó, model được xây dựng từ các linear layers, attentions mechanisms và normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kể từ tháng Một năm 2020, Transformers trở thành một kiến trúc nổi bật trong NLP, đạt được rất nhiều kết quả state-of-the-art trong nhiều tasks khác nhau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một trong những biến thể phổ biến nhất của Transformer là [*BERT*](https://arxiv.org/pdf/1810.04805.pdf) (**B**idirectional **E**ncoder **R**epresentantions from **T**ransformers) và phiên bản pre-trained BERT, chúng thường được sử dụng để thay thế embedding layers và làm một số tác vụ khác trong các NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một thư viện phổ biến được sử dụng khi làm việc với pre-trained transformers là [Transformers](https://huggingface.co/docs/transformers/index) library, nhấn vào [đây](https://huggingface.co/models) để biết thêm về các pre-trained models có sẵn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dưới đây là một số khác biệt trong notebook và paper: <br>\n",
    ">>> * Ở đây, chúng ta sẽ sử dụng các bộ positional encoding động (tức tự học) thay vì các bộ tĩnh.\n",
    ">>> * Bên cạnh đó, chúng ta cũng sử dụng Adam optimizer với learning rate tĩnh thay vì warm-up và cool-down steps.\n",
    ">>> * Cuối cùng, chúng ta không áp dụng label smoothing trong notebook này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta sẽ làm cho những thay đổi này gần với các set-up của BERT và phần lớn các biến thể của Transformer models đều sử dụng các set-up tương tự."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import các thư viện và thiết lập random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load các tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tạo các trường data. Lưu ý `batch_first = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '', \n",
    "            eos_token = '', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '', \n",
    "            eos_token = '', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load Multi30k dataset và build vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tạo Iterator và đưa vào device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tương tự như các notebook trước, model lần này cũng được tạo bởi 2 khối: encoder và decoder. Encoder thực hiện mã hóa input/source sentence (Tiếng Đức) thành *context vector* và, decoder thực hiện giải mã *context vector* thành ouput/target sentence (Tiếng Anh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tương tự như Convolutional Seq2Seq model, encoder của Transformer không nén toàn bộ source sentence, **X = ($x_1$, ..., $x_n$)** vào một context vector duy nhất, z. Thay vào đó, encoder của model sẽ sinh ra một chuỗi các context vectors **Z = ($z_1$, ..., $z_n$)**. Vậy, nếu input sequence có chiều dài là 5 tokens, chúng ta sẽ thu được chuỗi 5 context vectors **Z = ($z_1$, $z_2$, $z_3$, $z_4$, $z_5$)**. <br>\n",
    ">> **NOTE:** Chúng ta gọi sequence này là một *chuỗi các context vectors* thay vì *chuỗi các hidden state* là do: một hidden state tại thời điểm t trong RNN chỉ quan sát token $x_t$ và tất cả các tokens trước đó. Tuy nhiên, mỗi context vector được sinh ra bởi Transformer model đều có thể quan sát tất cả các tokens cũng vị trí của chúng trong input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>>> ![figure2](./images/6.encoder.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đầu tiên, các tokens được truyền vào một embedding layer. Tiếp theo, do model không sử dụng recurrent nên nó không thể nhận biết được thứ tự của các tokens bên trong sequence. Ta giải quyết vấn đề này bằng cách sử dụng embedding layer thứ 2, được gọi là *positional embedding layer*. Lớp positional embedding này nhận đầu vào không phải là token mà là vị trí của token trong sequence, bắt đầu với token đầu tiên \\<sos> token, ở vị trí 0. Position embedding có \"vocabulary\" với kích thước bằng 100, có nghĩa chúng ta có thể xử lý sequence có độ dài lên tới 100 tokens. Ta hoàn toàn có thể thay đổi tham số này nếu muốn xử lý sequence dài hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Phiên bản gốc của Transformer từ paper Attention is All You Need không học positional embedding. Thay vào đó, nó sử dụng embedding có định. Các kiến trúc Transformer hiện đại như BERT sử dụng positional embedding và đạt hiệu suất tốt hơn. Nhấn vào [đây](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) để đọc thêm về positional embedding được sử dụng trong mô hình Transformer gốc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, token và positional embeddings được *elementwise summed* với nhau để sinh ra vector chứa thông tin về token cũng như vị trí của chúng bên trong sequence. Tuy nhiên, truocs khi được cộng, token embedding được nhân với hệ số tỷ lệ $\\sqrt{d_{model}}$, với $d_{model}$ là hidden dimension size `hid_dim`. Ta thực hiện công việc scaling trên để giảm phương sai trong embedding. Nếu không có hệ số nhân này, model có vẻ không đáng tin cậy. Sau đó,ta áp dụng Dropout sao khi cộng hai vector token embeddings và positional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Embedding cùng được truyền qua N *encoder layers* để thu được Z, làm đầu vào cho decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Source mask, `src_mask` có kích thước bằng với source sentence nhưng có các phần tử chỉ bằng 0 hoặc 1 (1 khi token trong source sentence không phải là padding token và bằng 0 khi nó là padding token). Nó được sử dụng trong encoder layers để che đi multi-head attention mechanisms - cơ chế này được tính toán và áp dụng attention trên source sentence, do đó model không cần thiết phải chú ý đến padding token vì chúng không mang thông tin hữu ích."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 hid_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 pf_dim,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong encoder layer, đầu tiên chúng ta truyền source sentence và mask của nó vào *multi-head attention layer*, thực hiện dropout, áp dụng residual connection và truyền chúng qua [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) layer. Sau đó chúng ta truyền nó qua một *position-wise feedforward* layer và một lần nữa, áp dụng dropout, một residual connection và sau cùng normalization để thu được đầu ra của encoder layer này và đưa vào encoder layer tiếp theo. Lưu ý là parameters không được chia sẻ giữa các layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Multi head attention layer được sử dụng bởi encoder layer để tập trung chú ý vào source sentence, nó tính toán và áp dụng attention lên chính nó thay vì các sequence khác nên được gọi là *self-attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Về cơ bản, layer normalization là thực hiện chuẩn hóa giá trị thuộc tính, ví dụ theo hidden dimension, do đó mỗi thuộc tính có mean bằng 0, độ lệch chuẩn bằng 1. Việc thực hiện layer normalization giúp neural networks với nhiều layers như Transformer được train dễ dàng hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi Head Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một trong những ý tưởng hay, quan trọng nhất trong Transformer chính là *multi-head attention layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure3](./images/6.attention.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
