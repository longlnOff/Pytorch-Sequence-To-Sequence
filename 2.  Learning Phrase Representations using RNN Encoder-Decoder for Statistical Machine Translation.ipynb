{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook này, chúng ta sẽ cùng implementing model từ paper [ Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf). Model này sẽ cải thiện độ chính xác trên tập test một cách đáng kể trong khi chỉ sử dụng một RNN layer ở encoder và decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cùng nhìn lại kiến trúc chung của encoder-decoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure1](./images/2.seq2seq_model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Các tokens đầu vào đi qua embedding layer (yellow), sau đó đi vào encoder (green) để tạo ra *context vector* (red). Sau đó, *context vector* được đưa vào decoder (blue), sinh ra các vector và được đưa vào linear layer (purple) để sinh ra target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở notebook trước, chúng ta sử dụng multi-layered LSTM để xây dựng encoder và decoer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![figure2](./images/2.lstm_seq2seq.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nhược điểm của model trong notebook trước là decoder cố gắng nhồi nhét quá nhiều thông tin vào trong hidden states. Trong quá trình decoding, hidden state cần phải chứa TOÀN BỘ thông tin về source sequence, cũng như toàn bộ các tokens đã được decoded. Bằng cách giảm NHẸ lượng thông tin này, ta có thể tạo một model tốt hơn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook này, ta sẽ sử dụng GRU (Gated Recurrent Unit) thay cho LSTM (Long Short-Term Memory). Tại sao? Lý do chính là bài báo mà chúng ta khảo sát sẽ sử dụng GRUs, và notebook trước chúng ta đã dùng LSTMs rồi :v. Để hiểu về sự khác nhau giữa GRUs, LSTMs và standard RNNs, nhấn vào [đây](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Vậy GRUs hay LSTMs tốt hơn? [Nghiên cứu này]() chỉ ra rằng GRUs và LSTMs cho kết quả xấp xỉ nhau, và cả 2 đều tốt hơn standard RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta sẽ sử dụng torchtext để thực hiện các bước pre-processing và dùng spacy để tokenize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Chúng ta sẽ set random seeds để thu được kết quả đồng nhất ở các lần chạy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiêp theo, ta tạo tokenizers. Tokenizer được sử dụng để biến một string chứa các sentence thành một list các tokens riêng biệt tạo nên string đó. ví dụ: \"good morning!\" sau khi được tokenized trở thành [\"good\", \"morning\", \"!\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> spaCy có các models để tokenize cho các loại ngôn ngữ khác nhau. Với tiếng Đức ta có \"de_core_news_sm\", tiếng Anh là \"en_core_web_sm\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Các models tokenize cần được tải về. <br>\n",
    ">> * python -m spacy download en_core_web_sm <br>\n",
    ">> * python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta cần tạo hàm để tokenize các sentences. Hàm này sẽ có input là string, output là list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý: Hàm tokenize tiếng Đức sẽ đảo thứ tự list sau khi được tokenize do việc đâỏ ngược thứ tự các tokens giúp quá trình optimization dễ dàng hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (as tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (as tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Field`s trong torchtext sẽ xác định cách mà data được xử lý. Nhấn vào [đây]() để biết thêm chi tiết về `Field`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta cần set các đối số `tokenize` đúng với hàm tokenize của nó. Tiếng Đức tương ứng với biến `SRC`, tiếng Anh là `TRG`. Các trường cũng chèn thêm tokens 'start of sequence' và 'end of sequence' thông qua `init_token` và `eos_token`, đồng thời nó sẽ conert tất cả các từ sang lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de,\n",
    "            init_token = '',\n",
    "            eos_token = '',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            init_token = '',\n",
    "            eos_token = '',\n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, chúng ta sẽ tải và load train, test, validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset ở đây là [Multi30k dataset](https://github.com/multi30k/dataset). Dataset chứa ~30000 câu tiếng Anh, Đức, Pháp và mỗi câu có khoảng 12 từ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `exts` xác định ngôn ngữ nào được sử dụng cho source và target (source đứng trước) còn `fields` xác định field nào được sử dụng cho source và target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hiển thị một câu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': ['.',\n",
       "  'büsche',\n",
       "  'vieler',\n",
       "  'nähe',\n",
       "  'der',\n",
       "  'in',\n",
       "  'freien',\n",
       "  'im',\n",
       "  'sind',\n",
       "  'männer',\n",
       "  'weiße',\n",
       "  'junge',\n",
       "  'zwei'],\n",
       " 'trg': ['two',\n",
       "  'young',\n",
       "  ',',\n",
       "  'white',\n",
       "  'males',\n",
       "  'are',\n",
       "  'outside',\n",
       "  'near',\n",
       "  'many',\n",
       "  'bushes',\n",
       "  '.']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, chúng ta sẽ xây dựng *vocabulary* cho source và target languages. Vocabulary dùng để liên kết mỗi unique token với chỉ số của nó. Vocabulary của source và target languages là khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sử dụng đối số `min_freq = 2`, ta chỉ cho phép các tokens xuất hiện tối thiểu 2 lần được đưa vào vocabulary. Các tokens xuất hiện ít hơn 2 lần sẽ được coi là \\<unk> hay unknow token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một điều cần lưu ý nữa là ta chỉ xây dựng vocabulary cho training set, không xây cho validation/test set. Việc này giúp ngăn chặn vấn đề \"information leakage\" (information leakage khiến việc đánh giá không được khách quan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7854\n",
      "Unique tokens in target (en) vocabulary: 5892\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bước cuối cùng trong tiền xử lý data là tạo iterators. Đối tượng iterators này sẽ được lặp trong từng batch và có `src` attribute (là một Pytorch tensors chứa một batch các source sentences đã được số hóa - Numericalized), có `trg` attribute (là một Pytorch tensors chứa một batch các target sentences đã được số hóa - Numericalized). Numericalized là một cách nói đơn giản của việc chuyển các chuổi tokens có thể đọc được thanh chuối các indexes tương ứng, sử dụng vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khi ta train, trong 1 batch luôn yêu cầu các sentence phải có cùng độ dài, do đó ta cần padding các câu ngắn. May mắn thay, torchtext có các công cụ để xử lý vấn đề này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta sử dụng `BucketIterator` thay cho standard `Iterator` vì nó tạo các bathes theo cách tối ưu nhất - tối thiểu hóa số lượng padding token ở cả source và target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Encoder tương tự như notebook trước. Có 2 điểm khác biệt so với notebook trước là: LSTM được thay thế bởi single-layer GRU và chỉ sử dụng 1 layer thay vì 2 layers. Bên cạnh đó, ta sẽ không truyền tham số `dropout` vào RNN do chỉ sử dụng 1 lớp. Nếu cố gắng truyền `dropout` khi sử dụng 1 layer, Pytorch sẽ warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một điểm cần lưu ý khác là GRU chỉ yêu cầu hidden state, đồng thời nó cũng trả về hidden state, cell state không tồn tại trong GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dưới đây là biểu thức của GRU, LSTM và standard RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> * $\\bold{h_t}$ = $\\bold{GRU(e(x_t), h_{t-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> * $\\bold{(h_t, c_t)}$ = $\\bold{LSTM(e(x_t), (h_{t-1}, c_{t-1}))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> * $\\bold{h_t}$ = $\\bold{RNN(e(x_t), h_{t-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Từ biểu thức trên, ta có thể thấy RNN và GRU khá giống nhau. Tuy nhiên, bên trong GRU có một cơ chế gọi là *gating machanisms* điều khiển lượng thông tin vào và ra hidden state (tương tự như LSTM). Nhấn vào [đây](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) để đọc thêm về GRU và *gating machanisms*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Phần còn lại của encoder tương tự so với tutorial trước, nó nhận đầu vào là một sequence, $\\bold{X = \\{x_1, x_2, ..., x_T\\}}$ được truyền qua embedding layer, được tính hidden states $\\bold{H = \\{h_1, h_2, ..., h_T\\}}$ và trả về context vector (final hidden state), z = $h_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> $\\bold{h_t = EncoderGRU(e(x_t), h_{t-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Encoder được tạo bởi GRU tương tự như encoder của các seq2seq model thông thường."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gru](./images/2.seq2seq_gru.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_dim,\n",
    "                emb_dim,\n",
    "                hid_dim,\n",
    "                n_layers = 1,\n",
    "                dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        if n_layers > 1:\n",
    "            self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # outputs are always from the top hidden layer\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decoder trong notebook này sẽ có nhiều thay đổi so với decoder model ở notebook trước, và chúng ta sẽ giảm việc nén thông tin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GRU trong decoder sẽ nhận đầu vào là embedded target token, $\\bold{d(y_T)}$, previous hidden state $\\bold{s_{t-1}}$ và context vector z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> $\\bold{s_t = DecoderGRU(d(y_t), s_{t-1}, z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý là context vector z không có chỉ số $\\bold{t}$, có nghĩa là ta sẽ sử dụng CÙNG một context vector được trả về từ encoder trong suốt quá trình decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trước đây, chúng ta dự đoán token tiếp theo, $\\bold{\\hat{y}_{t+1}}$ bằng linear layer **f**, và chỉ sử dụng top-layer decoder hidden state ở thời điểm đó, $s_t$ như sau: <br>\n",
    ">> $\\hat{y}_{t+1}$ = f($s^L_t$) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Giờ đây, ta còn truyền thêm embedding của token hiện tại, $d(y_t)$ context vector z vào linear layer để dự đoán. Biểu thức như sau: <br>\n",
    ">> $\\hat{y}_{t+1}$ = f($d(y_t)$, $s^L_t$, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kiến trúc decoder của chúng ta sẽ trông như thế này:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > ![decoder](./images/2.seq2seq_decoder.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý, initial hidden state, $s_0$ vẫn là context vector z. Cho nên khi sinh token đầu tiên, thực chất chúng ta sẽ đưa 2 vector giống hệt nhau vào GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Câu hỏi đưa ra là: làm cách nào mà 2 thay đổi này giảm việc nén thông tin lại? Well, theo lý thuyết thì decoder hidden states $s_t$ không cần chứa thông tin về source sequence vì source sequence luôn có sẵn. Do đó, nó chỉ cần lưu trữ các thông tin về token nào đã được sinh ra tính đến thời điểm hiện tại. Việc thêm $\\bold{y_t}$ và linear layer cũng có nghĩa là layer này có thể trực tiếp quan sát token đó là gì mà không cần thêm thông tin từ hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tuy nhiên thì lý thuyết vẫn là lý thuyết, ta không có cách nào xác định được model có thực sự sử dụng thông tin được cung cấp hay không? Tuy nhiên, về mặt trực quan, ta có thể thấy rằng việc điều chỉnh model này là một good ideal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khi implementation GRU, ta truyền $d(y_t)$ và z vào bằng cách ghép nối chúng lại với nhau, cho nên input dimensions của GRU sẽ là `emb_dim + hid_dim` (do context vector sẽ có size là `hid_dim`). Linear layer sẽ nhận $d(y_t)$, $s_t$ và z là đầu vào, do đó ta cần concatenating chúng lại và ta có input dimensions sẽ là `emb_dim + hid_dim*2`. Và, đương nhiên là ta không cần truyền giá trị dropout vào GRu vì ở đây, ta chỉ sử dụng duy nhất 1 layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Giờ đây, hàm `forward` nhận thêm 2 đối số là `context` và `hidden`. Bên trong `forward`, ta ghép nối $y_t$ và z thành `emb_con` trước khi đưa vào GRU, sau đó ta concatenate d($y_t$), $s_t$ và z cùng nhau tạo thành `output` trước khi đưa nó vào linear layer để đưa ra dự đoán token, $\\hat{y}_{t+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                output_dim,\n",
    "                emb_dim,\n",
    "                hid_dim,\n",
    "                n_layers = 1,\n",
    "                dropout=0.5\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        if n_layers > 1:\n",
    "            self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # context = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # n layers and n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "        # context = [1, batch size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        # input = [1, batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        # embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "        # output = [seq len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "\n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
    "\n",
    "        # output = [batch size, emb dim + hid dim * 2]\n",
    "\n",
    "        prediction = self.fc_out(output)\n",
    "\n",
    "        # prediction = [batch size, output dim]\n",
    "\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ghép encoder và decoder vào với nhau, ta có:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ![model](./images/2.seq2seq_together.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khi implementation, ta cần đảm bảo hidden dimension ở encoder và decoder phải bằng nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tóm tắt nhanh các bước trong model: <br>\n",
    ">> * `outputs` tensor được tạo để lưu trữ tất cả các predictions, $\\bold{\\hat{Y}}$.\n",
    ">> * Source sequence $\\bold{X}$ được đưa vào encoder, từ đó ta thu được `context` vector.\n",
    ">> * Giá trị khởi tạo của decoder hidden state được gán bằng `context` vector, $s_0$ = z = $h_T$.\n",
    ">> * Ta sẽ sử dụng batch of \\<sos> tokens làm first `input`, $y_1$.\n",
    ">> * Sau đó, ta thực hiện decode với vòng lặp:\n",
    ">>>> * Nhét input token $y_t$, previous hidden state $s_{t-1}$ và context vector z vào decoder.\n",
    ">>>> * Lưu trữ giá trị dự đoán $\\hat{y}_{t+1}$ và new hidden state $s_t$.\n",
    ">>>> * Tiếp theo, ta sẽ kiểm tra điều kiện thực hiện teacher force, và gán next input với giá trị phù hợp (ground của next token trong target sequence hoặc token có giá trị dự đoán cao nhất)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        # src = [src len, batch size]\n",
    "        # trg = [trg len, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden = self.encoder(src)\n",
    "        context = hidden\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden state and the context state\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Phần còn lại của notebook này khá giống lần trước."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta khởi tạo encoder, decoder, seq2seq model và đồng thời đưa chúng vào GPU nếu có. Như đã nói từ trước, embedding dimensions và tỷ lệ dropout giữa encoder và decoder có thể khác nhau nhưng hidden dimensions phải giống nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta khởi tạo các parameters theo phân phối chuẩn $\\bold{N(0, 0.01)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7854, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5892, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=5892, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Đếm số lượng parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,432,068 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ta có thể thấy số lượng parameters nhiều hơn model trước mặc dù chỉ sử dụng một layer RNN. Lý do là ta đã tăng kích thước đầu vào của GRU và linear layer nên số lượng parameters tăng là điều đương nhiên. Tuy nhiên số lượng parameters này chỉ làm tăng thời gian train khoảng 3s/epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khởi tao optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khởi tạo loss function, lưu ý là ta cần ignore padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 22m 45s\n",
      "\tTrain Loss: 4.952 | Train PPL: 141.404\n",
      "\t Val. Loss: 5.012 |  Val. PPL: 150.155\n",
      "Epoch: 02 | Time: 18m 32s\n",
      "\tTrain Loss: 4.267 | Train PPL:  71.316\n",
      "\t Val. Loss: 5.144 |  Val. PPL: 171.344\n",
      "Epoch: 03 | Time: 18m 13s\n",
      "\tTrain Loss: 3.900 | Train PPL:  49.412\n",
      "\t Val. Loss: 4.558 |  Val. PPL:  95.345\n",
      "Epoch: 04 | Time: 18m 34s\n",
      "\tTrain Loss: 3.532 | Train PPL:  34.183\n",
      "\t Val. Loss: 4.242 |  Val. PPL:  69.550\n",
      "Epoch: 05 | Time: 18m 4s\n",
      "\tTrain Loss: 3.203 | Train PPL:  24.596\n",
      "\t Val. Loss: 3.968 |  Val. PPL:  52.898\n",
      "Epoch: 06 | Time: 18m 25s\n",
      "\tTrain Loss: 2.937 | Train PPL:  18.856\n",
      "\t Val. Loss: 3.909 |  Val. PPL:  49.868\n",
      "Epoch: 07 | Time: 18m 15s\n",
      "\tTrain Loss: 2.718 | Train PPL:  15.146\n",
      "\t Val. Loss: 3.742 |  Val. PPL:  42.173\n",
      "Epoch: 08 | Time: 18m 12s\n",
      "\tTrain Loss: 2.494 | Train PPL:  12.111\n",
      "\t Val. Loss: 3.733 |  Val. PPL:  41.811\n",
      "Epoch: 09 | Time: 18m 24s\n",
      "\tTrain Loss: 2.293 | Train PPL:   9.901\n",
      "\t Val. Loss: 3.710 |  Val. PPL:  40.834\n",
      "Epoch: 10 | Time: 17m 25s\n",
      "\tTrain Loss: 2.147 | Train PPL:   8.561\n",
      "\t Val. Loss: 3.728 |  Val. PPL:  41.615\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.702 | Test PPL:  40.545 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/tut2-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Với giá trị test loss, ta có thể tạm kết luận model này tốt hơn model trước. Đây là một dấu hiệu tốt khi đánh giá một kiến trúc model. Giảm việc nén thông tin có vẻ là một phương pháp tốt, và trong notebook tới, ta sẽ mở rộng nó với *attention mechanism*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
