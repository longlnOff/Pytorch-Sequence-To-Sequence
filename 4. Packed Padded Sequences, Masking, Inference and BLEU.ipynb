{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Packed Padded Sequences, Masking, Inference and BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Trong notebook này, chúng ta sẽ cải tiến model ở notebook trước - áp dụng packed padded sequences và masking. Packed padded sequences có chức năng thông báo cho RNN biết và bỏ qua các padding tokens trong encoder. Trong khi đó, masking \"ép\" model ignore nhũng giá trị cụ thể - ví dụ như attention score của các padded elements. cả 2 kỹ thuật trên (packed padded sequences và masking) đều được sử dụng khá phổ biến trong NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bên cạnh đó, chúng ta cũng xây dựng cách mô hình dự đoán - đầu vào là một câu, đầu ra sẽ là câu được dịch ở ngôn ngữ khác, đồng thời ta có thể quan sát điểm attention khi dịch mỗi từ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cuối cùng, ta sẽ sử dụng BLEU metric để đánh giá chất lượng model dịch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import spacy và định nghĩa German và English tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Khi sử dụng *packed padded sequences*, chúng ta cần thông báo với PyTorch độ dài thực (non-padded) của sequences là gì. May mắn thay, trường `Field` trong TorchText cho phép ta sử dụng đối số `include_lengths` và chuyển `batch.src` thành một tupple với: phần tử đầu tiên trong tupple là một batch các source sentences đã được số hóa dưới dạng tensor, và phần tử thứ 2 là chiều dài thực (non-padded) của mỗi source sentence trong batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de,\n",
    "            init_token='',\n",
    "            eos_token='',\n",
    "            lower = True,\n",
    "            include_lengths=True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            init_token='',\n",
    "            eos_token='',\n",
    "            lower = True,\n",
    "            include_lengths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Build Vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, chúng ta xử lý iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Một điểm không rõ ràng về *packed padded sequences* là tất cả các phần tử trong batch cần được sắp xếp bởi chiều dài thực của câu (non-padded lengths) theo thứ tự giảm dần. Ta sẽ sử dụng 2 đối số trong iterator để xử lý vấn đề này: `sort_with_batch` thông báo cho iterator biết nội dung trong batch cần được sắp xếp, và `sort_key` là một hàm xác định cách mà iterator sắp xếp các elements trong batch. Cụ thể trong trường hợp này, ta sắp xếp theo chiều dài của source sentence, `src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator =\\\n",
    "    BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tiếp theo, ta xây dựng encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So với model trước, model lần này chỉ có chút thay đổi trong `forward` method. Bây giờ, nó nhận 2 đối số là: chiều dài sentence và sentence thay vì 1 đối số như trước."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> sau khi soure sentence (sẽ được tự động đệm trong iterator) được embedded, chúng ta có thể sử dụng `pack_padded_sequence` lên nó cùng với chiều dài thực của mỗi câu. Lưu ý là tensor chứa chiều dài của câu phải được chuyển về CPU. Sau đó, `packed_embedded` sẽ là *packed padded sequence* và được đưa vào RNN như bình thường. RNN sẽ trả về 2 đại lượng:  `packed_outputs` - một packed tensor chứa tất cả các hidden states từ sequence, và `hidden` - là final hidden state từ sequence. `hidden` là một tensor chuẩn và không được packed, điểm khác biệt duy nhất là inut là một packed sequence, tensor này thu được từ một **non-padded element** trong sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sau đó, ta sẽ unpack `packed_outputs` với `pad_paced_sequence` - thứ sẽ trả về `outputs` và chiều dài của chúng (mỗi output có 1 chiều dài)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dimension đầu tiên trong `outputs` là chiều dài của padded sequence, tuy nhiên ta sử dụng *packed padded sequence* nên giá trị của các tensors đầu tiên trong `outputs` bằng 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_dim, \n",
    "                emb_dim, \n",
    "                enc_hid_dim, \n",
    "                dec_hid_dim, \n",
    "                dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "\n",
    "        # src = [src len, batch size]\n",
    "        # src_len = [batch size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        # need to pack the sequence to feed to RNN, and put lengths to cpu\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        # packed_outputs is a packed sequence containing all hidden states\n",
    "        # hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        # outputs is now a non-packed sequence, all hidden states obtained\n",
    "        # when the input is a pad token are all zeros\n",
    "            \n",
    "        # outputs = [src len, batch size, hid dim * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        \n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        # encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        # outputs = [src len, batch size, enc hid dim * 2]\n",
    "        # hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Attetion module dùng để tính attention values của source sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ở model trước, chúng ta cho phép attention module \"chú ý\" tới các padding tokens bên trong source sentence. Tuy nhiên, lần này chúng ta sẽ sử dụng *masking* để buộc attetion module chỉ hoạt động đối với các non-padding elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('python3.7.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94b83b56f38ffef65d6a4ee563210b313d606c3429660e46922f3bd794e4159a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
